{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization: TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With enormous amount of data surrounding us, it is important to be able to extract the most important information from it. In this notebook, we focus on one such information extraction algorithm from text. \n",
    "\n",
    "Broadly speaking, there are two different approaches which one can take for summarizing text: extractive summarization where the summary of the document is a part of the document itself and abstractive summarization where the summary is not a part of the document and is generated by a learning model. \n",
    "\n",
    "Abstractive summarization is an extremely difficult problem and to date an area of cuntinued research. Some of the advancements in abstractive summarization using recurrant neural networks can be found in the works mentioned in [Quora](https://www.quora.com/Has-Deep-Learning-been-applied-to-automatic-text-summarization-successfully). \n",
    "\n",
    "In this work, we approach text summarization from an extractive viewpoint. The questions which we address are: \n",
    "   * Given a document, which are the most important lines in it?   \n",
    "   * Given a document, which are the most important key-words in it?  \n",
    "\n",
    "To answer these questions, we implement TextRank which is an algorithm that ranks text in a document based on the importance of the text. TextRank is analogous to Google's PageRank and was introduced by Mihalcea and Tarau in the paper [TextRank: Bringing Order into Texts](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf).\n",
    "\n",
    "TextRank is an unsupervised learning algorithm and much simpler to implement as compared to abstractive summarization methods and yet yields good Recall-Oriented Understudy for Gisting Evaluation (ROGUE) scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import argparse\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_sentences(doc):\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    return sentence_tokenizer.tokenize(doc)\n",
    "\n",
    "def remove_non_words(sentences):\n",
    "    regex = re.compile('[^a-zA-Z\\\" \"]')\n",
    "    return [regex.sub('', s) for s in sentences]\n",
    "\n",
    "def get_idx_to_sentences(sentences):\n",
    "    return {idx: s for idx, s in enumerate(sentences)}\n",
    "\n",
    "def get_idx_to_word(vocab):\n",
    "    return {vocab[word]: word for word in vocab}\n",
    "\n",
    "def get_ranks(directed_graph_weights, d=0.85):\n",
    "    A = directed_graph_weights\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0\n",
    "        col_sum = np.sum(A[:,id])\n",
    "        if col_sum != 0:\n",
    "            A[:, id] /= col_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "    \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    \n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "def display_highlighted_sentneces(ranks_of_sentences, \n",
    "                                  raw_sentneces, \n",
    "                                  sentences_to_highlight = 3,\n",
    "                                  dark=0.8):\n",
    "    sorted_sentences_ranks_idx = sorted(ranks_of_sentences, key=lambda k: ranks_of_sentences[k], reverse=True)\n",
    "    weights = [ranks_of_sentences[idx] for idx in ranks_of_sentences]\n",
    "    weights = (weights - min(weights))/(max(weights) - min(weights) + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for idx in range(len(raw_sentences)):\n",
    "        if idx in sorted_sentences_ranks_idx[:sentences_to_highlight]:\n",
    "            c = int(256*((1.-dark)*(1.-ranks_of_sentences[idx])+dark))\n",
    "        else:\n",
    "            c = int(256*((1.-dark)*(1.-0)+dark))    \n",
    "        html += fmt.format(c,raw_sentences[idx])\n",
    "    display(HTML(html))\n",
    "    \n",
    "def display_highlighted_words(ranks_of_words, \n",
    "                              raw_sentences, \n",
    "                              vocab,\n",
    "                              words_to_highlight = 10,\n",
    "                              dark=0.8):\n",
    "    weights = [ranks_of_words[idx] for idx in ranks_of_words]\n",
    "    sorted_words_ranks_idx = sorted(ranks_of_words, key=lambda k: ranks_of_words[k], reverse=True)\n",
    "    weights = (weights - min(weights))/(max(weights) - min(weights) + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for s in raw_sentences:\n",
    "        for w_ in s.split(' '):\n",
    "            regex = re.compile('[^a-zA-Z\\\" \"]')\n",
    "            w = regex.sub('', w_)\n",
    "            if len(PorterTokenizer().__call__(w))!=0:\n",
    "                stemmed_word = PorterTokenizer().__call__(w)[0].lower()\n",
    "            else:\n",
    "                stemmed_word = \" \"\n",
    "            if stemmed_word in vocab and vocab[stemmed_word] in sorted_words_ranks_idx[:words_to_highlight]:\n",
    "                c = int(256*((1.-dark)*(1.-ranks_of_words[vocab[stemmed_word]])+dark))\n",
    "            else:\n",
    "                c = int(256*((1.-dark)*(1.-0)+dark))\n",
    "            html += fmt.format(c,w_)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('TextRank')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"Accumulation of intracellular double-stranded RNA (dsRNA) usually marks viral \" \\\n",
    "    \"infections or de-repression of endogenous retroviruses and repeat elements. The innate \" \\\n",
    "    \"immune system, the first line of defense in mammals, is therefore equipped to sense \" \\\n",
    "    \"dsRNA and mount a protective response. The largest family of dsRNA sensors are \" \\\n",
    "    \"oligoadenylate synthetases (OAS) which produce a second messenger, 2-5A, in \" \\\n",
    "    \"response to dsRNA. This 2-5A activates an endoribonuclease, RNase L, which cleaves \" \\\n",
    "    \"single-stranded cellular and viral RNAs. OAS/RNase L is not only essential for coping \" \\\n",
    "    \"with bacterial and viral infections but also a major regulator of cell cycle progression, \" \\\n",
    "    \"differentiation, and apoptosis, processes often misregulated in cancers. We seek to \" \\\n",
    "    \"understand the dynamics and molecular basis of signaling in the OAS/RNase L \" \\\n",
    "    \"pathway. To this end we have developed a three-pronged approach to: a) identify \" \\\n",
    "    \"dsRNAs that accumulate b) monitor 2-5A levels real-time in live cells and c) map direct \" \\\n",
    "    \"RNA cleavages by RNase L. These approaches collectively provide a complete \" \\\n",
    "    \"molecular framework to examine dsRNA signaling in various infections and disease \" \\\n",
    "    \"states.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = \"We present a novel tool called XGossip for Internet-scale cardinality estimation of \" \\\n",
    "    \"XPath queries over distributed XML data. XGossip relies on the principle of gossip, is \" \\\n",
    "    \"scalable, decentralized, and can cope with network churn and failures. It employs a novel \" \\\n",
    "    \"divide-and-conquer strategy for load balancing and reducing the overall network bandwidth \" \\\n",
    "    \"consumption. It has a strong theoretical underpinning and provides provable guarantees on \" \\\n",
    "    \"the accuracy of cardinality estimates, the number of messages exchanged, and the total \" \\\n",
    "    \"bandwidth usage. In this demonstration, users will experience three engaging scenarios: \" \\\n",
    "    \"In the first scenario, they can set up, configure, and deploy XGossip on Amazon Elastic \" \\\n",
    "    \"Compute Cloud (EC2). In the second scenario, they can execute XGossip, pose XPath queries, \" \\\n",
    "    \"observe in real-time the convergence speed of XGossip, the accuracy of cardinality estimates, \"\\\n",
    "    \"the bandwidth usage, and the number of messages exchanged. In the third scenario, they can \" \\\n",
    "    \"introduce network churn and failures during the execution of XGossip and observe how these \"\\\n",
    "    \"impact the behavior of XGossip.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_doc = \"Quantum mechanics is interesting. Quantum mechanics is weird. Hello, you there?\"\n",
    "document = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "simple_doc commentry: For illustrative purpose, we will use the above simple_doc to show the steps involved in the implementation of TextRank. The following things should be noted about this document:\n",
    "   * It is clear that the third sentence is not something important. So, we expect that the third sentence should be ranked lowest by TextRank. \n",
    "   * It is not clear whether the first or the second sentence is more important. \n",
    "   * It is clear that \"quantum\" and \"mechanics\" are the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_sentences = get_sentences(document) # From the document, extract the list of sentences\n",
    "sentences = remove_non_words(raw_sentences) # Remove all non-words from raw_sentences\n",
    "idx_to_sentences = get_idx_to_sentences(sentences) # Get index to sentences \n",
    "\n",
    "logger.debug(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A callable class which stems the word to its root according to the rules defined in ProterStemmer\n",
    "class PorterTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return [self.porter.stem(word) for word in args[0].split()]\n",
    "    \n",
    "logger.debug(PorterTokenizer().__call__(\"run running runs\")) # Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a term frequency-inverse-document-frequency vectorizer object\n",
    "# Input: List of sentences.\n",
    "# Processing: 1) Remove stop words defined in stop_words from the sentences and \n",
    "#             2) Stem the words to its roots according to PorterStemmer\n",
    "tfidf = TfidfVectorizer(preprocessor=None, \n",
    "                        stop_words=stopwords.words('english'),\n",
    "                        tokenizer=PorterTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tfidf_mat: Normalized tfidf matrix with each row corresponding to a sentence and each column corresponding to a word \n",
    "# vocab: Dictionary of words and its corresponding index. \n",
    "#        The index coresponds to the column number of the word in tfidf_mat \n",
    "tfidf_mat = tfidf.fit_transform(sentences).toarray()\n",
    "vocab = tfidf.vocabulary_\n",
    "idx_to_word = get_idx_to_word(vocab)\n",
    "logger.debug('\\n{}'.format(tfidf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple_doc commentry: We see from the above that there are 5 words which make up our vocabulary and there are three sentences. Notice that the words \"you\" and \"there\", which were part of the third sentence: \"Hello you there?\", have been removed from the vocabulary by stop_words. As a result of this, only \"hello\" remains in the third sentence. This is confirmed by the fact that for the third sentence (third row), we have 1 in the 0th column (note that in vocab, 'hello': 0) of tfidf_mat and all other columns are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct directed weighed graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of carrying out the algorithm of TextRank, we now construct a directed weighed graph where each sentence is a node and the edges between two sentences specify the similarity between them. Suppose s_i corresponds to tfidf vector for sentence i (that is the i_th row in tfidf_mat), then the similarity between sentence i and j is defined as s_i * s_j.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directed_graph_weights_sentences: A square matrix with dimension (num_of_sentences x num_of_sentences).\n",
    "#                                 : This matrix is symmetric. \n",
    "#                                 : (i,j)th element of the matrix specifies the similarity between sentences i and j. \n",
    "directed_graph_weights_sentences = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "\n",
    "logger.debug('\\n{}'.format(directed_graph_weights_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to defining the weight graph for sentences, we can define a weight graph for the words in the document. The similarity between words i and j is defined as s_i.T * s_j where s_i and s_j are sentence rows in tfidf_mat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directed_graph_weights_words: A square matrix with dimension (num_of_words_in_vocab x num_of_words_in_vocab).\n",
    "#                             : This matrix is symmetric. \n",
    "#                             : (i,j)th element of the matrix specifies the similarity between words i and j. \n",
    "directed_graph_weights_words = np.dot(tfidf_mat.T, tfidf_mat)\n",
    "\n",
    "logger.debug('\\n{}'.format(directed_graph_weights_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranks of sentences/words using PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the graph weights, we solve for the ranks of the sentences and words in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranks_of_sentences = get_ranks(directed_graph_weights_sentences, 0.85)\n",
    "ranks_of_words = get_ranks(directed_graph_weights_words, 0.85)\n",
    "\n",
    "logger.debug(ranks_of_sentences)\n",
    "logger.debug(ranks_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilighted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #cfcfff\">We present a novel tool called XGossip for Internet-scale cardinality estimation of XPath queries over distributed XML data.</span> <span style=\"background-color: #100100ff\">XGossip relies on the principle of gossip, is scalable, decentralized, and can cope with network churn and failures.</span> <span style=\"background-color: #100100ff\">It employs a novel divide-and-conquer strategy for load balancing and reducing the overall network bandwidth consumption.</span> <span style=\"background-color: #100100ff\">It has a strong theoretical underpinning and provides provable guarantees on the accuracy of cardinality estimates, the number of messages exchanged, and the total bandwidth usage.</span> <span style=\"background-color: #100100ff\">In this demonstration, users will experience three engaging scenarios: In the first scenario, they can set up, configure, and deploy XGossip on Amazon Elastic Compute Cloud (EC2).</span> <span style=\"background-color: #a2a2ff\">In the second scenario, they can execute XGossip, pose XPath queries, observe in real-time the convergence speed of XGossip, the accuracy of cardinality estimates, the bandwidth usage, and the number of messages exchanged.</span> <span style=\"background-color: #babaff\">In the third scenario, they can introduce network churn and failures during the execution of XGossip and observe how these impact the behavior of XGossip.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_highlighted_sentneces(ranks_of_sentences, raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #100100ff\">We</span> <span style=\"background-color: #100100ff\">present</span> <span style=\"background-color: #100100ff\">a</span> <span style=\"background-color: #bcbcff\">novel</span> <span style=\"background-color: #100100ff\">tool</span> <span style=\"background-color: #100100ff\">called</span> <span style=\"background-color: #7272ff\">XGossip</span> <span style=\"background-color: #100100ff\">for</span> <span style=\"background-color: #100100ff\">Internet-scale</span> <span style=\"background-color: #adadff\">cardinality</span> <span style=\"background-color: #adadff\">estimation</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #bebeff\">XPath</span> <span style=\"background-color: #bebeff\">queries</span> <span style=\"background-color: #100100ff\">over</span> <span style=\"background-color: #100100ff\">distributed</span> <span style=\"background-color: #100100ff\">XML</span> <span style=\"background-color: #100100ff\">data.</span> <span style=\"background-color: #7272ff\">XGossip</span> <span style=\"background-color: #100100ff\">relies</span> <span style=\"background-color: #100100ff\">on</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">principle</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #100100ff\">gossip,</span> <span style=\"background-color: #100100ff\">is</span> <span style=\"background-color: #100100ff\">scalable,</span> <span style=\"background-color: #100100ff\">decentralized,</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">can</span> <span style=\"background-color: #100100ff\">cope</span> <span style=\"background-color: #100100ff\">with</span> <span style=\"background-color: #ababff\">network</span> <span style=\"background-color: #bebeff\">churn</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">failures.</span> <span style=\"background-color: #100100ff\">It</span> <span style=\"background-color: #100100ff\">employs</span> <span style=\"background-color: #100100ff\">a</span> <span style=\"background-color: #bcbcff\">novel</span> <span style=\"background-color: #100100ff\">divide-and-conquer</span> <span style=\"background-color: #100100ff\">strategy</span> <span style=\"background-color: #100100ff\">for</span> <span style=\"background-color: #100100ff\">load</span> <span style=\"background-color: #100100ff\">balancing</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">reducing</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">overall</span> <span style=\"background-color: #ababff\">network</span> <span style=\"background-color: #acacff\">bandwidth</span> <span style=\"background-color: #100100ff\">consumption.</span> <span style=\"background-color: #100100ff\">It</span> <span style=\"background-color: #100100ff\">has</span> <span style=\"background-color: #100100ff\">a</span> <span style=\"background-color: #100100ff\">strong</span> <span style=\"background-color: #100100ff\">theoretical</span> <span style=\"background-color: #100100ff\">underpinning</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">provides</span> <span style=\"background-color: #100100ff\">provable</span> <span style=\"background-color: #100100ff\">guarantees</span> <span style=\"background-color: #100100ff\">on</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">accuracy</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #adadff\">cardinality</span> <span style=\"background-color: #adadff\">estimates,</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">number</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #100100ff\">messages</span> <span style=\"background-color: #100100ff\">exchanged,</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">total</span> <span style=\"background-color: #acacff\">bandwidth</span> <span style=\"background-color: #100100ff\">usage.</span> <span style=\"background-color: #100100ff\">In</span> <span style=\"background-color: #100100ff\">this</span> <span style=\"background-color: #100100ff\">demonstration,</span> <span style=\"background-color: #100100ff\">users</span> <span style=\"background-color: #100100ff\">will</span> <span style=\"background-color: #100100ff\">experience</span> <span style=\"background-color: #100100ff\">three</span> <span style=\"background-color: #100100ff\">engaging</span> <span style=\"background-color: #9292ff\">scenarios:</span> <span style=\"background-color: #100100ff\">In</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">first</span> <span style=\"background-color: #9292ff\">scenario,</span> <span style=\"background-color: #100100ff\">they</span> <span style=\"background-color: #100100ff\">can</span> <span style=\"background-color: #100100ff\">set</span> <span style=\"background-color: #100100ff\">up,</span> <span style=\"background-color: #100100ff\">configure,</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">deploy</span> <span style=\"background-color: #7272ff\">XGossip</span> <span style=\"background-color: #100100ff\">on</span> <span style=\"background-color: #100100ff\">Amazon</span> <span style=\"background-color: #100100ff\">Elastic</span> <span style=\"background-color: #100100ff\">Compute</span> <span style=\"background-color: #100100ff\">Cloud</span> <span style=\"background-color: #100100ff\">(EC2).</span> <span style=\"background-color: #100100ff\">In</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">second</span> <span style=\"background-color: #9292ff\">scenario,</span> <span style=\"background-color: #100100ff\">they</span> <span style=\"background-color: #100100ff\">can</span> <span style=\"background-color: #100100ff\">execute</span> <span style=\"background-color: #7272ff\">XGossip,</span> <span style=\"background-color: #100100ff\">pose</span> <span style=\"background-color: #bebeff\">XPath</span> <span style=\"background-color: #bebeff\">queries,</span> <span style=\"background-color: #100100ff\">observe</span> <span style=\"background-color: #100100ff\">in</span> <span style=\"background-color: #100100ff\">real-time</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">convergence</span> <span style=\"background-color: #100100ff\">speed</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #7272ff\">XGossip,</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">accuracy</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #adadff\">cardinality</span> <span style=\"background-color: #adadff\">estimates,</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #acacff\">bandwidth</span> <span style=\"background-color: #100100ff\">usage,</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">number</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #100100ff\">messages</span> <span style=\"background-color: #100100ff\">exchanged.</span> <span style=\"background-color: #100100ff\">In</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">third</span> <span style=\"background-color: #9292ff\">scenario,</span> <span style=\"background-color: #100100ff\">they</span> <span style=\"background-color: #100100ff\">can</span> <span style=\"background-color: #100100ff\">introduce</span> <span style=\"background-color: #ababff\">network</span> <span style=\"background-color: #bebeff\">churn</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">failures</span> <span style=\"background-color: #100100ff\">during</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">execution</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #7272ff\">XGossip</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #100100ff\">observe</span> <span style=\"background-color: #100100ff\">how</span> <span style=\"background-color: #100100ff\">these</span> <span style=\"background-color: #100100ff\">impact</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">behavior</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #7272ff\">XGossip.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_highlighted_words(ranks_of_words, raw_sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * In this notebook we implemented TextRank using tfidf matrix as the basis for similarity measure.  \n",
    "   * This extractive summarization method can be the starting point for abstractive summarization. For example, suppose we have a document having around 100 sentences. Making an abstractive model (say by using recurrent neural network) which can take these 100 sentences as input, and \"generate\" a headline from these 100 sentences is extremely challenging because of the large input space. This inout space can be dramatically reduced by first using TextRank to extract, say 5, most important sentneces from the document and then using these 5 sentences as input to the abstractive summarizer model.  \n",
    "   * In this work, I did not test how well the model performs. I tried using pyrouge, but got into configuration (ini) issues. \n",
    "   * For key-word ranking, it was noted in the original TextRank paper that it is better to use only nouns and adjectives as possible candidates. In this work, besides using sentence tokanization and removing stop words, I did not use any grammar specific knowledge. It will be interesting to implement grammar based filtering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization: TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With enormous amount of data surrounding us, it is important to be able to extract the most important information from it. In this notebook, we focus on one such information extraction algorithm from text. \n",
    "\n",
    "Broadly speaking, there are two different approaches which one can take for summarizing text: extractive summarization where the summary of the document is a part of the document itself and abstractive summarization where the summary is not a part of the document and is generated by a learning model. \n",
    "\n",
    "Abstractive summarization is an extremely difficult problem and to date an area of cuntinued research. Some of the advancements in abstractive summarization using recurrant neural networks can be found in the works mentioned in [Quora](https://www.quora.com/Has-Deep-Learning-been-applied-to-automatic-text-summarization-successfully). \n",
    "\n",
    "In this work, we approach text summarization from an extractive viewpoint. The questions which we address are: \n",
    "   * Given a document, which are the most important lines in it?   \n",
    "   * Given a document, which are the most important key-words in it?  \n",
    "\n",
    "To answer these questions, we implement TextRank which is an algorithm that ranks text in a document based on the importance of the text. TextRank is analogous to Google's PageRank and was introduced by Mihalcea and Tarau in the paper [TextRank: Bringing Order into Texts](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf).\n",
    "\n",
    "TextRank is an unsupervised learning algorithm and much simpler to implement as compared to abstractive summarization methods and yet yields good Recall-Oriented Understudy for Gisting Evaluation (ROGUE) scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import argparse\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_sentences(doc):\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    return sentence_tokenizer.tokenize(doc)\n",
    "\n",
    "def remove_non_words(sentences):\n",
    "    regex = re.compile('[^a-zA-Z\\\" \"]')\n",
    "    return [regex.sub('', s) for s in sentences]\n",
    "\n",
    "def get_idx_to_sentences(sentences):\n",
    "    return {idx: s for idx, s in enumerate(sentences)}\n",
    "\n",
    "def get_idx_to_word(vocab):\n",
    "    return {vocab[word]: word for word in vocab}\n",
    "\n",
    "def get_ranks(directed_graph_weights, d=0.85):\n",
    "    A = directed_graph_weights\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0\n",
    "        col_sum = np.sum(A[:,id])\n",
    "        if col_sum != 0:\n",
    "            A[:, id] /= col_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "    \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    \n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "def display_highlighted_sentneces(ranks_of_sentences, \n",
    "                                  raw_sentneces, \n",
    "                                  sentences_to_highlight = 3,\n",
    "                                  dark=0.8):\n",
    "    sorted_sentences_ranks_idx = sorted(ranks_of_sentences, key=lambda k: ranks_of_sentences[k], reverse=True)\n",
    "    weights = [ranks_of_sentences[idx] for idx in ranks_of_sentences]\n",
    "    weights = (weights - min(weights))/(max(weights) - min(weights) + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for idx in range(len(raw_sentences)):\n",
    "        if idx in sorted_sentences_ranks_idx[:sentences_to_highlight]:\n",
    "            c = int(256*((1.-dark)*(1.-ranks_of_sentences[idx])+dark))\n",
    "        else:\n",
    "            c = int(256*((1.-dark)*(1.-0)+dark))    \n",
    "        html += fmt.format(c,raw_sentences[idx])\n",
    "    display(HTML(html))\n",
    "    \n",
    "def display_highlighted_words(ranks_of_words, \n",
    "                              raw_sentences, \n",
    "                              vocab,\n",
    "                              words_to_highlight = len(vocab)/5,\n",
    "                              dark=0.8):\n",
    "    weights = [ranks_of_words[idx] for idx in ranks_of_words]\n",
    "    sorted_words_ranks_idx = sorted(ranks_of_words, key=lambda k: ranks_of_words[k], reverse=True)\n",
    "    weights = (weights - min(weights))/(max(weights) - min(weights) + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for s in raw_sentences:\n",
    "        for w_ in s.split(' '):\n",
    "            regex = re.compile('[^a-zA-Z\\\" \"]')\n",
    "            w = regex.sub('', w_)\n",
    "            stemmed_word = PorterTokenizer().__call__(w)[0].lower()\n",
    "            if stemmed_word in vocab and vocab[stemmed_word] in sorted_words_ranks_idx[:words_to_highlight]:\n",
    "                c = int(256*((1.-dark)*(1.-ranks_of_words[vocab[stemmed_word]])+dark))\n",
    "            else:\n",
    "                c = int(256*((1.-dark)*(1.-0)+dark))\n",
    "            html += fmt.format(c,w_)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('TextRank')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"Accumulation of intracellular double-stranded RNA (dsRNA) usually marks viral \" \\\n",
    "    \"infections or de-repression of endogenous retroviruses and repeat elements. The innate \" \\\n",
    "    \"immune system, the first line of defense in mammals, is therefore equipped to sense \" \\\n",
    "    \"dsRNA and mount a protective response. The largest family of dsRNA sensors are \" \\\n",
    "    \"oligoadenylate synthetases (OAS) which produce a second messenger, 2-5A, in \" \\\n",
    "    \"response to dsRNA. This 2-5A activates an endoribonuclease, RNase L, which cleaves \" \\\n",
    "    \"single-stranded cellular and viral RNAs. OAS/RNase L is not only essential for coping \" \\\n",
    "    \"with bacterial and viral infections but also a major regulator of cell cycle progression, \" \\\n",
    "    \"differentiation, and apoptosis, processes often misregulated in cancers. We seek to \" \\\n",
    "    \"understand the dynamics and molecular basis of signaling in the OAS/RNase L \" \\\n",
    "    \"pathway. To this end we have developed a three-pronged approach to: a) identify \" \\\n",
    "    \"dsRNAs that accumulate b) monitor 2-5A levels real-time in live cells and c) map direct \" \\\n",
    "    \"RNA cleavages by RNase L. These approaches collectively provide a complete \" \\\n",
    "    \"molecular framework to examine dsRNA signaling in various infections and disease \" \\\n",
    "    \"states.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_doc = \"Quantum mechanics is interesting. Quantum mechanics is weird. Hello, you there?\"\n",
    "document = simple_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "simple_doc commentry: For illustrative purpose, we will use the above simple_doc to show the steps involved in the implementation of TextRank. The following things should be noted about this document:\n",
    "   * It is clear that the third sentence is not something important. So, we expect that the third sentence should be ranked lowest by TextRank. \n",
    "   * It is not clear whether the first or the second sentence is more important. \n",
    "   * it is clear that \"quantum\" and \"mechanics\" are the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_sentences = get_sentences(document) # From the document, extract the list sentences\n",
    "sentences = remove_non_words(raw_sentences) # Remove all non-words from sentence\n",
    "idx_to_sentences = get_idx_to_sentences(sentences) # Get index to sentences \n",
    "\n",
    "logger.debug(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A callable class which stems the word to its root according to the rules defined in ProterStemmer\n",
    "class PorterTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return [self.porter.stem(word) for word in args[0].split()]\n",
    "    \n",
    "logger.debug(PorterTokenizer().__call__(\"run running runs\")) # Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a term frequence-inverse document frequence vectorizer object\n",
    "# Input: List of sentneces.\n",
    "# Processing: 1) Remove stop words defined in stop_words from the sentences and \n",
    "#             2) Stem the words to its roots according to PorterStemmer\n",
    "tfidf = TfidfVectorizer(preprocessor=None, \n",
    "                        stop_words=stopwords.words('english'),\n",
    "                        tokenizer=PorterTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mat: Normalized tfidf matrix with each row corresponding to a sentence and each column corresponding to a word \n",
    "# vocab: Dictionary of words and its corresponding index. The index coresponds to the column number of the word in mat \n",
    "tfidf_mat = tfidf.fit_transform(sentences).toarray()\n",
    "vocab = tfidf.vocabulary_\n",
    "idx_to_word = get_idx_to_word(vocab)\n",
    "\n",
    "logger.debug('\\n{}'.format(tfidf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple_doc commentry: We see from the above that there are 5 words which make up our vocabulary and there are three sentences. Notice that the words \"you\" and \"there\", which were part of the third sentence: \"Hello you there?\", has been removed from the vocabulary by stop_words. As a result of this, only \"hello\" remains in the third sentence. This is confirmed by the fact that for the third sentence (third row), we have 1 in the 0th column (note that in vocab, 'hello': 0) of tfidf_mat and all other columns are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of carrying out the algorithm of TextRank, we now construct a directed weighed graph where each sentence is a node and the edges between two sentences specify the similarity between them. Suppose s_i corresponds to tfidf vector for sentence i (that is the i_th row in tfidf_mat), then the similarity between sentence i and j is defined as s_i * s_j.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directed_graph_weights_sentences = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "logger.debug('\\n{}'.format(directed_graph_weights_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to defining the weight graph for sentences, we can define a weight graph for the words in the document. The similarity between words i and j is defined as s_i.T * s_j where s_i and s_j are sentence rows in tfidf_mat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directed_graph_weights_words = np.dot(tfidf_mat.T, tfidf_mat)\n",
    "logger.debug('\\n{}'.format(directed_graph_weights_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the graph weights, we solve for the ranks of the sentences and words in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranks_of_sentences = get_ranks(directed_graph_weights_sentences, 0.85)\n",
    "ranks_of_words = get_ranks(directed_graph_weights_words, 0.85)\n",
    "\n",
    "logger.debug(ranks_of_sentences)\n",
    "logger.debug(ranks_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #ccccff\">Quantum mechanics is interesting.</span> <span style=\"background-color: #ccccff\">Quantum mechanics is weird.</span> <span style=\"background-color: #f8f8ff\">Hello, you there?</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_highlighted_sentneces(ranks_of_sentences, raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #c0c0ff\">Quantum</span> <span style=\"background-color: #c0c0ff\">mechanics</span> <span style=\"background-color: #100100ff\">is</span> <span style=\"background-color: #d9d9ff\">interesting.</span> <span style=\"background-color: #c0c0ff\">Quantum</span> <span style=\"background-color: #c0c0ff\">mechanics</span> <span style=\"background-color: #100100ff\">is</span> <span style=\"background-color: #d9d9ff\">weird.</span> <span style=\"background-color: #f8f8ff\">Hello,</span> <span style=\"background-color: #100100ff\">you</span> <span style=\"background-color: #100100ff\">there?</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_highlighted_words(ranks_of_words, raw_sentences, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

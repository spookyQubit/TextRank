{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization: TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the enormous amount of data surrounding us, it is important to be able to extract the most important information from it. In this notebook, we focus on one such information extraction algorithm from text. \n",
    "\n",
    "Broadly speaking, there are two different approaches for summarizing text: 1) extractive summarization, where the summary is lifted verbatim from the document itself and 2) abstractive summarization, where the summary is not a part of the document but is synthesized de novo by a learning model. \n",
    "\n",
    "Abstractive summarization is an extremely difficult problem and remains an area of active research. Some of the recent advancements employ recurrant neural networks and can be found in the works mentioned in [Quora](https://www.quora.com/Has-Deep-Learning-been-applied-to-automatic-text-summarization-successfully).\n",
    "\n",
    "Here, I approach text summarization from an extractive viewpoint, tackling two specific questions: \n",
    "   * Given a document, which are the most important sentences?   \n",
    "   * Given a document, what are the key words?  \n",
    "\n",
    "To answer these questions, I implement TextRank - a graph-based algorithm that ranks text in a document based on the importance of the text. TextRank is analogous to Google's PageRank and was introduced by Mihalcea and Tarau in the paper [TextRank: Bringing Order into Texts](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf).\n",
    "\n",
    "TextRank is an unsupervised learning algorithm and much simpler to implement as compared to abstractive summarization and yet yields good Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import argparse\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    return sentence_tokenizer.tokenize(doc)\n",
    "\n",
    "def remove_non_words(sentences):\n",
    "    regex = re.compile('[^a-zA-Z\\\" \"]')\n",
    "    return [regex.sub('', s) for s in sentences]\n",
    "\n",
    "def get_idx_to_sentences(sentences):\n",
    "    return {idx: s for idx, s in enumerate(sentences)}\n",
    "\n",
    "def get_idx_to_word(vocab):\n",
    "    return {vocab[word]: word for word in vocab}\n",
    "\n",
    "def get_ranks(directed_graph_weights, d=0.85):\n",
    "    A = directed_graph_weights\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0\n",
    "        col_sum = np.sum(A[:,id])\n",
    "        if col_sum != 0:\n",
    "            A[:, id] /= col_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "    \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    \n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "def display_highlighted_sentneces(ranks_of_sentences, \n",
    "                                  raw_sentneces, \n",
    "                                  sentences_to_highlight = 3,\n",
    "                                  dark=0.8):\n",
    "    sorted_sentences_ranks_idx = sorted(ranks_of_sentences, key=lambda k: ranks_of_sentences[k], reverse=True)\n",
    "    weights = [ranks_of_sentences[idx] for idx in ranks_of_sentences]\n",
    "    weights = (weights - min(weights))/(max(weights) - min(weights) + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for idx in range(len(raw_sentences)):\n",
    "        if idx in sorted_sentences_ranks_idx[:sentences_to_highlight]:\n",
    "            c = int(256*((1.-dark)*(1.-ranks_of_sentences[idx])+dark))\n",
    "        else:\n",
    "            c = int(256*((1.-dark)*(1.-0)+dark))    \n",
    "        html += fmt.format(c,raw_sentences[idx])\n",
    "    display(HTML(html))\n",
    "    \n",
    "def display_highlighted_words(ranks_of_words, \n",
    "                              raw_sentences, \n",
    "                              vocab,\n",
    "                              words_to_highlight = 10,\n",
    "                              dark=0.8):\n",
    "    weights = [ranks_of_words[idx] for idx in ranks_of_words]\n",
    "    sorted_words_ranks_idx = sorted(ranks_of_words, key=lambda k: ranks_of_words[k], reverse=True)\n",
    "    weights = (weights - min(weights))/(max(weights) - min(weights) + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for s in raw_sentences:\n",
    "        for w_ in s.split(' '):\n",
    "            regex = re.compile('[^a-zA-Z\\\" \"]')\n",
    "            w = regex.sub('', w_)\n",
    "            if len(PorterTokenizer().__call__(w))!=0:\n",
    "                stemmed_word = PorterTokenizer().__call__(w)[0].lower()\n",
    "            else:\n",
    "                stemmed_word = \" \"\n",
    "            if stemmed_word in vocab and vocab[stemmed_word] in sorted_words_ranks_idx[:words_to_highlight]:\n",
    "                c = int(256*((1.-dark)*(1.-ranks_of_words[vocab[stemmed_word]])+dark))\n",
    "            else:\n",
    "                c = int(256*((1.-dark)*(1.-0)+dark))\n",
    "            html += fmt.format(c,w_)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('TextRank')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_doc = \"Quantum mechanics is interesting. Quantum mechanics is weird. Hello, you there?\"\n",
    "document = simple_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "simple_doc commentry: For illustrative purpose, we will use the above simple_doc to show the steps involved in the implementation of TextRank. The following things should be noted about this document:\n",
    "   * It is clear that the third sentence is not something important. So, we expect that the third sentence should be ranked lowest by TextRank. \n",
    "   * It is not clear whether the first or the second sentence is more important. \n",
    "   * It is clear that \"quantum\" and \"mechanics\" are the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:TextRank:['Quantum mechanics is interesting', 'Quantum mechanics is weird', 'Hello you there']\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = get_sentences(document) # From the document, extract the list of sentences\n",
    "sentences = remove_non_words(raw_sentences) # Remove all non-words from raw_sentences\n",
    "idx_to_sentences = get_idx_to_sentences(sentences) # Get index to sentences \n",
    "\n",
    "logger.debug(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:TextRank:[u'run', u'run', u'run']\n"
     ]
    }
   ],
   "source": [
    "# A callable class which stems the word to its root according to the rules defined in ProterStemmer\n",
    "class PorterTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return [self.porter.stem(word) for word in args[0].split()]\n",
    "    \n",
    "logger.debug(PorterTokenizer().__call__(\"run running runs\")) # Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a term frequency-inverse-document-frequency vectorizer object\n",
    "# Input: List of sentences.\n",
    "# Processing: 1) Remove stop words defined in stop_words from the sentences and \n",
    "#             2) Stem the words to its roots according to PorterStemmer\n",
    "tfidf = TfidfVectorizer(preprocessor=None, \n",
    "                        stop_words=stopwords.words('english'),\n",
    "                        tokenizer=PorterTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:TextRank:\n",
      "[[ 0.    0.68  0.52  0.52  0.  ]\n",
      " [ 0.    0.    0.52  0.52  0.68]\n",
      " [ 1.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# tfidf_mat: Normalized tfidf matrix with each row corresponding to a sentence and each column corresponding to a word \n",
    "# vocab: Dictionary of words and its corresponding index. \n",
    "#        The index coresponds to the column number of the word in tfidf_mat \n",
    "tfidf_mat = tfidf.fit_transform(sentences).toarray()\n",
    "\n",
    "logger.debug('\\n{}'.format(tfidf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple_doc commentry: We see from the above that there are 5 words which make up our vocabulary and there are three sentences. Notice that the words \"you\" and \"there\", which were part of the third sentence: \"Hello you there?\", have been removed from the vocabulary by stop_words. As a result of this, only \"hello\" remains in the third sentence. This is confirmed by the fact that for the third sentence (third row), we have 1 in the 0th column (note that in vocab, 'hello': 0) of tfidf_mat and all other columns are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:TextRank:\n",
      "{0: u'hello', 1: u'interest', 2: u'mechan', 3: u'quantum', 4: u'weird'}\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(preprocessor=None, \n",
    "                        stop_words=stopwords.words('english'),\n",
    "                        tokenizer=PorterTokenizer())\n",
    "cv_mat = normalize(cv.fit_transform(sentences).toarray().astype(float), axis=0)\n",
    "vocab = cv.vocabulary_\n",
    "idx_to_word = get_idx_to_word(vocab)\n",
    "\n",
    "logger.debug('\\n{}'.format(idx_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct directed weighed graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of carrying out the algorithm of TextRank, we now construct a directed weighed graph where each sentence is a node and the edges between two sentences specify the similarity between them. Suppose s_i corresponds to tfidf vector for sentence i (that is the i_th row in tfidf_mat), then the similarity between sentence i and j is defined as s_i * s_j.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:TextRank:\n",
      "[[ 1.    0.54  0.  ]\n",
      " [ 0.54  1.    0.  ]\n",
      " [ 0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# directed_graph_weights_sentences: A square matrix with dimension (num_of_sentences x num_of_sentences).\n",
    "#                                 : This matrix is symmetric. \n",
    "#                                 : (i,j)th element of the matrix specifies the similarity between sentences i and j. \n",
    "directed_graph_weights_sentences = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "\n",
    "logger.debug('\\n{}'.format(directed_graph_weights_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to defining the weight graph for sentences, we can define a weight graph for the words in the document. The similarity between words i and j is defined as s_i.T * s_j where s_i and s_j are sentence rows in tfidf_mat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:TextRank:\n",
      "[[ 1.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.    0.71  0.71  0.  ]\n",
      " [ 0.    0.71  1.    1.    0.71]\n",
      " [ 0.    0.71  1.    1.    0.71]\n",
      " [ 0.    0.    0.71  0.71  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# directed_graph_weights_words: A square matrix with dimension (num_of_words_in_vocab x num_of_words_in_vocab).\n",
    "#                             : This matrix is symmetric. \n",
    "#                             : (i,j)th element of the matrix specifies the similarity between words i and j. \n",
    "directed_graph_weights_words = np.dot(cv_mat.T, cv_mat)\n",
    "\n",
    "logger.debug('\\n{}'.format(directed_graph_weights_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranks of sentences/words using PageRank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the graph weights, we solve for the ranks of the sentences and words in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:TextRank:{0: 1.0, 1: 1.0, 2: 0.15000000000000002}\n",
      "DEBUG:TextRank:{0: 0.15000000000000002, 1: 0.76495280978071989, 2: 1.2350471902192799, 3: 1.2350471902192799, 4: 0.76495280978071978}\n"
     ]
    }
   ],
   "source": [
    "ranks_of_sentences = get_ranks(directed_graph_weights_sentences, 0.85)\n",
    "ranks_of_words = get_ranks(directed_graph_weights_words, 0.85)\n",
    "\n",
    "logger.debug(ranks_of_sentences)\n",
    "logger.debug(ranks_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilighted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #ccccff\">Quantum mechanics is interesting.</span> <span style=\"background-color: #ccccff\">Quantum mechanics is weird.</span> <span style=\"background-color: #f8f8ff\">Hello, you there?</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_highlighted_sentneces(ranks_of_sentences, raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #c0c0ff\">Quantum</span> <span style=\"background-color: #c0c0ff\">mechanics</span> <span style=\"background-color: #100100ff\">is</span> <span style=\"background-color: #d8d8ff\">interesting.</span> <span style=\"background-color: #c0c0ff\">Quantum</span> <span style=\"background-color: #c0c0ff\">mechanics</span> <span style=\"background-color: #100100ff\">is</span> <span style=\"background-color: #d8d8ff\">weird.</span> <span style=\"background-color: #f8f8ff\">Hello,</span> <span style=\"background-color: #100100ff\">you</span> <span style=\"background-color: #100100ff\">there?</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_highlighted_words(ranks_of_words, raw_sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * In this notebook we implemented TextRank using tfidf matrix as the basis for similarity measure.  \n",
    "   * This extractive summarization method can be the starting point for abstractive summarization. For example, suppose we have a document having around 100 sentences. Making an abstractive model (say by using recurrent neural network) which can take these 100 sentences as input, and \"generate\" a headline from these 100 sentences is extremely challenging because of the large input space. This inout space can be dramatically reduced by first using TextRank to extract, say 5, most important sentneces from the document and then using these 5 sentences as input to the abstractive summarizer model.  \n",
    "   * In this work, I did not test how well the model performs. I tried using pyrouge, but got into configuration (ini) issues. \n",
    "   * For key-word ranking, it was noted in the original TextRank paper that it is better to use only nouns and adjectives as possible candidates. In this work, besides using sentence tokanization and removing stop words, I did not use any grammar specific knowledge. It will be interesting to implement grammar based filtering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
